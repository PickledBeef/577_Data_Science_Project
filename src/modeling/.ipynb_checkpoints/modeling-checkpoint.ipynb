{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "path = Path(\"./modeling.ipynb\")\n",
    "abs_path = str(path.parent.absolute())\n",
    "index = [i for i, e in enumerate(abs_path) if e == \"\\\\\"]      \n",
    "parent_path = abs_path[:index[-2]]\n",
    "train_path = parent_path + \"/data/processed/train.csv\"\n",
    "test_path = parent_path + \"/data/processed/test.csv\"\n",
    "\n",
    "data = pd.read_csv(train_path)\n",
    "final_test = pd.read_csv(test_path)\n",
    "y = data['SalePrice']\n",
    "test_id = final_test['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size = 0.05, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "tf_imputer = ColumnTransformer([(\"imputer\", SimpleImputer(missing_values=np.nan, strategy = 'mean'), [0,1,2,3,4,5,6])])\n",
    "\n",
    "pre_pipeline = Pipeline(steps = ([(\"tf_imputer\", tf_imputer),\n",
    "                                ('scaler', MinMaxScaler(feature_range = (0,1)))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods are considered:\n",
    "1. K-nearest-neighbors, \n",
    "2. Decision tree, \n",
    "3. Random forest, \n",
    "4. 2 SVM methods (using a polynomial kernel and a Gaussian kernel),\n",
    "5. 2 (deep) neural networks with Sigmoid activation and ReLu activation functions\n",
    "6. Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_base = clone(pre_pipeline)\n",
    "knn_base.steps.append(('knn', KNeighborsClassifier()))\n",
    "knn_base.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn_base.predict(X_test)\n",
    "r2_base = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Performance for KNN baseline model is:\")\n",
    "print(\"---------------------------------------------\")\n",
    "print('R2 score is {}'.format(r2_base))\n",
    "print('MAE is {}'.format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = clone(pre_pipeline)\n",
    "pipeline.steps.append(('knn', KNeighborsClassifier()))\n",
    "param_dist = {\"knn__n_neighbors\": list(np.linspace(2, 7, 6, dtype = int)),\n",
    "              \"knn__leaf_size\": list(np.linspace(5, 150, 5, dtype = int)),\n",
    "              \"knn__weights\": [\"uniform\", \"distance\"],\n",
    "              \"knn__p\": [1, 2, 3]}\n",
    "random_search = RandomizedSearchCV(estimator = pipeline, param_distributions = param_dist, n_jobs = -1, verbose = 2, cv = 3)\n",
    "random_search.fit(X_train, y_train)\n",
    "knn_improve = random_search.best_estimator_\n",
    "\n",
    "y_pred = knn_improve.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "improve =  (r2 - r2_base) / r2_base\n",
    "print(\"Performance for KNN tuned model is:\")\n",
    "print(\"---------------------------------------------\")\n",
    "print('R2 score is {}'.format(r2))\n",
    "print('MAE is {}'.format(mae))\n",
    "print('Improvement of R2 score: {:0.2f}%.'.format( 100 * improve))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "dt_base = clone(pre_pipeline)\n",
    "dt_base.steps.append(('dt', tree.DecisionTreeClassifier(random_state = 42)))\n",
    "dt_base.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dt_base.predict(X_test)\n",
    "r2_base = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Performance for Decision Tree baseline model is:\")\n",
    "print(\"---------------------------------------------\")\n",
    "print('R2 score is {}'.format(r2_base))\n",
    "print('MAE is {}'.format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = clone(pre_pipeline)\n",
    "pipeline.steps.append(('dt', tree.DecisionTreeClassifier(random_state = 42)))\n",
    "param_dist = {\"dt__criterion\": [\"gini\", \"entropy\"],\n",
    "              \"dt__splitter\": [\"best\", \"random\"],\n",
    "              \"dt__max_features\": [\"auto\", \"sqrt\", \"log2\", None]}\n",
    "random_search = RandomizedSearchCV(estimator = pipeline, param_distributions = param_dist, n_jobs = -1, verbose = 2, cv = 3)\n",
    "random_search.fit(X_train, y_train)\n",
    "dt_improve = random_search.best_estimator_\n",
    "\n",
    "y_pred = dt_improve.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "improve =  (r2 - r2_base) / r2_base\n",
    "print(\"Performance for Decision Tree tuned model is:\")\n",
    "print(\"---------------------------------------------\")\n",
    "print('R2 score is {}'.format(r2))\n",
    "print('MAE is {}'.format(mae))\n",
    "print('Improvement of R2 score: {:0.2f}%.'.format( 100 * improve))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "rf_base = clone(pre_pipeline)\n",
    "rf_base.steps.append(('rf', ensemble.RandomForestClassifier(random_state = 42)))\n",
    "rf_base.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_base.predict(X_test)\n",
    "r2_base = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Performance for Random Forest baseline model is:\")\n",
    "print(\"---------------------------------------------\")\n",
    "print('R2 score is {}'.format(r2_base))\n",
    "print('MAE is {}'.format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = clone(pre_pipeline)\n",
    "pipeline.steps.append(('rf', ensemble.RandomForestClassifier(random_state = 42)))\n",
    "param_dist = {\"rf__n_estimators\": list(np.linspace(10, 2000, 20, dtype = int)),\n",
    "              \"rf__criterion\": [\"gini\", \"entropy\"],\n",
    "              \"rf__max_depth\": [3,4,5,6,7,8,9,10,11,12,13,None],\n",
    "              \"rf__max_features\": [\"auto\", \"sqrt\", \"log2\"]}\n",
    "random_search = RandomizedSearchCV(estimator = pipeline, param_distributions = param_dist, n_jobs = -1, verbose = 2, cv = 3)\n",
    "random_search.fit(X_train, y_train)\n",
    "rf_improve = random_search.best_estimator_\n",
    "\n",
    "y_pred = rf_improve.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "improve =  (r2 - r2_base) / r2_base\n",
    "print(\"Performance for Random Forest tuned model is:\")\n",
    "print(\"---------------------------------------------\")\n",
    "print('R2 score is {}'.format(r2))\n",
    "print('MAE is {}'.format(mae))\n",
    "print('Improvement of R2 score: {:0.2f}%.'.format( 100 * improve))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_base = clone(pre_pipeline)\n",
    "svm_base.steps.append(('svm', SVC(random_state = 42)))\n",
    "svm_base.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm_base.predict(X_test)\n",
    "r2_base = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Performance for SVM baseline model is:\")\n",
    "print(\"---------------------------------------------\")\n",
    "print('R2 score is {}'.format(r2_base))\n",
    "print('MAE is {}'.format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = clone(pre_pipeline)\n",
    "pipeline.steps.append(('svm', SVC(random_state = 42)))\n",
    "param_dist = {\"svm__kernel\": [\"poly\", \"rbf\"],\n",
    "              \"svm__degree\": [2,3,4,5],\n",
    "              \"svm__C\": list(np.linspace(1.0, 20.0, 5, dtype = float)),\n",
    "              \"svm__gamma\": [\"scale\", \"auto\"] }\n",
    "random_search = RandomizedSearchCV(estimator = pipeline, param_distributions = param_dist, n_jobs = -1, verbose = 2, cv = 3)\n",
    "random_search.fit(X_train, y_train)\n",
    "svm_improve = random_search.best_estimator_\n",
    "\n",
    "y_pred = svm_improve.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "improve =  (r2 - r2_base) / r2_base\n",
    "print(\"Performance for SVM tuned model is:\")\n",
    "print(\"---------------------------------------------\")\n",
    "print('R2 score is {}'.format(r2))\n",
    "print('MAE is {}'.format(mae))\n",
    "print('Improvement of R2 score: {:0.2f}%.'.format( 100 * improve))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "nn_base = clone(pre_pipeline)\n",
    "nn_base.steps.append(('nn', MLPClassifier(random_state = 42)))\n",
    "nn_base.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nn_base.predict(X_test)\n",
    "r2_base = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Performance for Neural Network baseline model is:\")\n",
    "print(\"---------------------------------------------\")\n",
    "print('R2 score is {}'.format(r2_base))\n",
    "print('MAE is {}'.format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = clone(pre_pipeline)\n",
    "pipeline.steps.append(('nn', MLPClassifier(random_state = 42)))\n",
    "param_dist = {\"nn__hidden_layer_sizes\": [(100,),(100,5),(100,2)],\n",
    "              \"nn__activation\":[\"logistic\", \"relu\"],\n",
    "              \"nn__solver\": [\"lbfgs\", \"sgd\", \"adam\"],\n",
    "              \"nn__learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"],\n",
    "              \"nn__max_iter\": list(np.linspace(100, 1500, 10, dtype = int)),\n",
    "              \"nn__early_stopping\": [True, False]}\n",
    "random_search = RandomizedSearchCV(estimator = pipeline, param_distributions = param_dist, n_jobs = -1, verbose = 2, cv = 5)\n",
    "random_search.fit(X_train, y_train)\n",
    "nn_improve = random_search.best_estimator_\n",
    "\n",
    "y_pred = nn_improve.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "improve =  (r2 - r2_base) / r2_base\n",
    "print(\"Performance for SVM tuned model is:\")\n",
    "print(\"---------------------------------------------\")\n",
    "print('R2 score is {}'.format(r2))\n",
    "print('MAE is {}'.format(mae))\n",
    "print('Improvement of R2 score: {:0.2f}%.'.format( 100 * improve))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T07:26:48.018307Z",
     "start_time": "2021-05-12T07:26:48.000390Z"
    }
   },
   "source": [
    "XGBoost is an open-source software library which provides a regularizing gradient boosting framework for C++, Java, Python, R, Julia, Perl, and Scala.\n",
    "\n",
    "According to answers from the question: [Is it necessary to scale the target value in addition to scaling features for regression analysis?](https://stats.stackexchange.com/questions/111467/is-it-necessary-to-scale-the-target-value-in-addition-to-scaling-features-for-re), I may try normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as XGB\n",
    "xgb_base = clone(pre_pipeline)\n",
    "xgb_base.steps.append(('xgb', XGB.XGBRegressor(random_state = 42)))\n",
    "xgb_base.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb_base.predict(X_test)\n",
    "r2_base = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Performance for Xgboost baseline model is:\")\n",
    "print(\"---------------------------------------------\")\n",
    "print('R2 score is {}'.format(r2_base))\n",
    "print('MAE is {}'.format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = clone(pre_pipeline)\n",
    "pipeline.steps.append(('xgb', XGB.XGBRegressor(random_state = 42)))\n",
    "param_dist = {\"xgb__n_estimators\": list(np.linspace(10, 3000, 20, dtype = int)),\n",
    "              \"xgb__max_depth\":list(np.linspace(5, 30, 25, dtype = int)),\n",
    "              \"xgb__learning_rate\": list(np.linspace(0.0, 1.0, 40, dtype = float)),\n",
    "              \"xgb__gamma\": list(np.linspace(0.0, 10.0, 40, dtype = float))}\n",
    "random_search = RandomizedSearchCV(estimator = pipeline, param_distributions = param_dist, n_jobs = -1, verbose = 2, cv = 3)\n",
    "random_search.fit(X_train, y_train)\n",
    "xgb_improve = random_search.best_estimator_\n",
    "\n",
    "y_pred = xgb_improve.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "improve =  (r2 - r2_base) / r2_base\n",
    "print(\"Performance for Xgboost tuned model is:\")\n",
    "print(\"---------------------------------------------\")\n",
    "print('R2 score is {}'.format(r2))\n",
    "print('MAE is {}'.format(mae))\n",
    "print('Improvement of R2 score: {:0.2f}%.'.format( 100 * improve))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_table = {\"KNN\": [0.418, 0.6966016612071357, 0.8092504113885084], \n",
    "                 \"Decision Tree\": [0.52, 0.6856229361149115, 0.6789868125230183],\n",
    "                 \"Random Forest\": [185, 0.766357912256961, 0.8158580683249272],\n",
    "                 \"SVM\": [20.2, 0.5395299138520958, 0.5105028066065995], \n",
    "                 \"Neural Network\": [525, 0.7474809530041818, 0.861390335441664],\n",
    "                 \"Xgboost\": [123, 0.8821370550045167, 0.8885990109061078]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cmp = pd.DataFrame(compare_table, dtype=float, \n",
    "            index = [\"Time cost for Randomized Search (seconds)\", \"R2 score for baseline model\", \"R2 score for tuned model\"])\n",
    "df_cmp.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cmp.T[\"Time cost for Randomized Search (seconds)\"].plot.bar(figsize=(10,8), rot=0, title=\"Time cost for Randomized Search (seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cmp.T[[\"R2 score for baseline model\", \"R2 score for tuned model\"]].plot.bar(figsize=(12,12), rot=0, title=\"R2 score for models with/without tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "        Combine both figures above, there're two choices after balancing time cost and prediction accuarcy:<br>\n",
    "            &nbsp;&nbsp;&nbsp;&nbsp;1. KNN: more time saving <br>\n",
    "            &nbsp;&nbsp;&nbsp;&nbsp;2. Xgboost: more accurate prediction <br>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = xgb_improve.predict(final_test)\n",
    "output = pd.DataFrame()\n",
    "output['Id'] = test_id\n",
    "output['SalePrice'] = y_predict\n",
    "output_path = parent_path + \"/data/output/submission.csv\"\n",
    "output.to_csv(output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
